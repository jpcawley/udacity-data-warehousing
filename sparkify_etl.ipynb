{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Set up AWS Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Python packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import create_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Get credentials from configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "ARN = config.get('IAM_ROLE', 'ARN')\n",
    "HOST = config.get('CLUSTER', 'HOST')\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "\n",
    "DB_NAME = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT = config.get(\"CLUSTER\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create AWS Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2')\n",
    "s3 = boto3.resource('s3')\n",
    "iam = boto3.client('iam')\n",
    "redshift = boto3.client('redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create IAM role with read access to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.1 Attaching a Policy\n",
      "1.3 Get the IAM role ARN\n",
      "IAM Role created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::488216229776:role/sparkifyRole'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIamRole():\n",
    "    '''Create IAM role.'''\n",
    "    try:\n",
    "        print('1.1 Creating a new IAM Role')\n",
    "        sparkifyRole = iam.create_role(\n",
    "            Path='/',\n",
    "            RoleName=ARN,\n",
    "            Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                                'Effect': 'Allow',\n",
    "                                'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                    'Version': '2012-10-17'})\n",
    "        )\n",
    "        print('1.1 Attaching a Policy')\n",
    "        iam.attach_role_policy(RoleName=ARN,\n",
    "                               PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                               )['ResponseMetadata']['HTTPStatusCode']\n",
    "        print('1.3 Get the IAM role ARN')\n",
    "        roleArn = iam.get_role(RoleName=ARN)['Role']['Arn']\n",
    "\n",
    "        print('IAM Role created successfully.')\n",
    "        return roleArn\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "createIamRole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Launch redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"sparkifycluster\".\n",
      "Cluster creation in progress.\n"
     ]
    }
   ],
   "source": [
    "def createRedshiftCluster():\n",
    "    roleArn = iam.get_role(RoleName=ARN)['Role']['Arn']\n",
    "    print('Creating cluster \"{}\".'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "    try:\n",
    "        response = redshift.create_cluster(        \n",
    "\n",
    "            ClusterType=DWH_CLUSTER_TYPE,\n",
    "            NodeType=DWH_NODE_TYPE,\n",
    "            NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "            DBName=DB_NAME,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DB_USER,\n",
    "            MasterUserPassword=DB_PASSWORD,\n",
    "\n",
    "            IamRoles=[roleArn],\n",
    "            \n",
    "            PubliclyAccessible=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    status = response['Cluster']['ClusterStatus']\n",
    "\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "#     print(myClusterProps)\n",
    "    time.sleep(20)\n",
    "    print('Cluster creation in progress.')\n",
    "    while True:\n",
    "        if status == 'creating':\n",
    "            time.sleep(10)\n",
    "            myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "            status = myClusterProps['ClusterStatus']\n",
    "        elif status == 'available':\n",
    "            sparkifyHost = myClusterProps['Endpoint']['Address']\n",
    "            print('sparkifyHost: ', sparkifyHost)\n",
    "            print('Created Redshift cluster \"{}\".'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "            break\n",
    "            \n",
    "createRedshiftCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Open a TCP port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def openTCP():\n",
    "    try:\n",
    "        vpcId = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]['VpcId']\n",
    "        vpc = ec2.Vpc(id=vpcId)\n",
    "        defaultSg = list(vpc.security_groups.all())[0]\n",
    "        print(defaultSg)\n",
    "        defaultSg.authorize_ingress(\n",
    "            GroupName=defaultSg.group_name,\n",
    "            CidrIp='0.0.0.0/0',\n",
    "            IpProtocol='TCP',\n",
    "            FromPort=int(DB_PORT),\n",
    "            ToPort=int(DB_PORT)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "openTCP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline: create fact and dimension tables then load data from S3 into staging tables on Redshift for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Connect to 'sparkifydb' and create the necessary tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_tables.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Clean up AWS resources: run the following cells ONLY when all ETL processing is complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Delete IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def deleteIamResources(iamRole):\n",
    "    '''Delete all created IAM resources.'''\n",
    "    iam.detach_role_policy(RoleName=iamRole, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    iam.delete_role(RoleName=iamRole)\n",
    "    print('Deleted IAM Role Resources.')\n",
    "    return\n",
    "\n",
    "deleteIamResources(ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Delete redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def deleteRedshiftResources(iamRole):\n",
    "    '''Delete created Redshift resources.'''\n",
    "    try:\n",
    "        print('Cluster deletion in progress.')\n",
    "        redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "            if not redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER):\n",
    "                break \n",
    "  \n",
    "    except Exception:\n",
    "    # except botocore.errorfactory.ClusterNotFoundFault:\n",
    "    # except botocore.exceptions.ClientError:\n",
    "        print('The cluster \"{}\" no longer exists.'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "        \n",
    "deleteRedshiftResources(ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Set up AWS Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Python packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import create_tables\n",
    "import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Get credentials from configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "ARN = config.get('IAM_ROLE', 'ARN')\n",
    "HOST = config.get('CLUSTER', 'HOST')\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "\n",
    "DB_NAME = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT = config.get(\"CLUSTER\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create AWS Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2')\n",
    "s3 = boto3.resource('s3')\n",
    "iam = boto3.client('iam')\n",
    "redshift = boto3.client('redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create IAM role with read access to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.1 Attaching a Policy\n",
      "1.3 Get the IAM role ARN\n",
      "IAM Role created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::488216229776:role/sparkifyRole'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createIamRole():\n",
    "    '''Create IAM role.'''\n",
    "    try:\n",
    "        print('1.1 Creating a new IAM Role')\n",
    "        sparkifyRole = iam.create_role(\n",
    "            Path='/',\n",
    "            RoleName=ARN,\n",
    "            Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                                'Effect': 'Allow',\n",
    "                                'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                    'Version': '2012-10-17'})\n",
    "        )\n",
    "        print('1.1 Attaching a Policy')\n",
    "        iam.attach_role_policy(RoleName=ARN,\n",
    "                               PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                               )['ResponseMetadata']['HTTPStatusCode']\n",
    "        print('1.3 Get the IAM role ARN')\n",
    "        roleArn = iam.get_role(RoleName=ARN)['Role']['Arn']\n",
    "\n",
    "        print('IAM Role created successfully.')\n",
    "        return roleArn\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "createIamRole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Launch redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster \"sparkifycluster\".\n",
      "Cluster creation is still in progress, checking status every 10 seconds.\n",
      "sparkifyHost:  sparkifycluster.ci6me8yds14k.us-east-1.redshift.amazonaws.com\n",
      "Created Redshift cluster \"sparkifycluster\".\n"
     ]
    }
   ],
   "source": [
    "def createRedshiftCluster():\n",
    "    roleArn = iam.get_role(RoleName=ARN)['Role']['Arn']\n",
    "    print('Creating cluster \"{}\".'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "    try:\n",
    "        response = redshift.create_cluster(        \n",
    "\n",
    "            ClusterType=DWH_CLUSTER_TYPE,\n",
    "            NodeType=DWH_NODE_TYPE,\n",
    "            NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "            DBName=DB_NAME,\n",
    "            ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DB_USER,\n",
    "            MasterUserPassword=DB_PASSWORD,\n",
    "\n",
    "            IamRoles=[roleArn],\n",
    "            \n",
    "            PubliclyAccessible=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    status = response['Cluster']['ClusterStatus']\n",
    "\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "#     print(myClusterProps)\n",
    "    time.sleep(20)\n",
    "    print('Cluster creation is still in progress, checking status every 10 seconds.')\n",
    "    while True:\n",
    "        if status == 'creating':\n",
    "            time.sleep(10)\n",
    "            myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "            status = myClusterProps['ClusterStatus']\n",
    "        elif status == 'available':\n",
    "            sparkifyHost = myClusterProps['Endpoint']['Address']\n",
    "            print('sparkifyHost: ', sparkifyHost)\n",
    "            print('Created Redshift cluster \"{}\".'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "            break\n",
    "            \n",
    "createRedshiftCluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Open TCP Port with 'default' security group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-05390f6b6bd310aa7')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "def openTCP():\n",
    "    try:\n",
    "        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "        vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "        defaultSg = list(vpc.security_groups.all())[1]\n",
    "        print(defaultSg)\n",
    "        defaultSg.authorize_ingress(\n",
    "            GroupName=defaultSg.group_name,\n",
    "            CidrIp='0.0.0.0/0',\n",
    "            IpProtocol='TCP',\n",
    "            FromPort=int(DB_PORT),\n",
    "            ToPort=int(DB_PORT)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "openTCP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline: create fact and dimension tables then load data from S3 into staging tables on Redshift for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Connect to 'sparkifydb' and create the necessary tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_tables.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Confirm table creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, HOST, DB_PORT, DB_NAME)\n",
    "print(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "nSongplays = %sql select count(*) from songplays;\n",
    "nUsers = %sql select count(*) from users;\n",
    "nSongs = %sql select count(*) from songs;\n",
    "nArtists = %sql select count(*) from artists;\n",
    "nTime = %sql select count(*) from time;\n",
    "nEventsStaging = %sql select count(*) from events_staging;\n",
    "nSongsStaging = %sql select count(*) from songs_staging;\n",
    "\n",
    "print(\"nSongplays\\t\\t=\", nSongplays[0][0])\n",
    "print(\"nUsers\\t=\", nUsers[0][0])\n",
    "print(\"nSongs\\t=\", nSongs[0][0])\n",
    "print(\"nArtists\\t=\", nArtists[0][0])\n",
    "print(\"nTime\\t\\t=\", nTime[0][0])\n",
    "print(\"nEventsStaging\\t\\t=\", nEventsStaging[0][0])\n",
    "print(\"nSongsStaging\\t\\t=\", nSongsStaging[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### read log_json_path.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "resp = s3.get_object(Bucket='udacity-dend', Key='log_json_path.json')\n",
    "content = resp['Body'].read()\n",
    "json = json.loads(content)\n",
    "print(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load staging tables and database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "QueryCanceledError",
     "evalue": "Query cancelled on user's request\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueryCanceledError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b04e6d9ee8c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0metl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/workspace/etl.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mload_staging_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minsert_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/workspace/etl.py\u001b[0m in \u001b[0;36mload_staging_tables\u001b[0;34m(cur, conn)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_staging_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcopy_table_queries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mQueryCanceledError\u001b[0m: Query cancelled on user's request\n"
     ]
    }
   ],
   "source": [
    "etl.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from stl_load_errors order by starttime desc limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Clean up AWS resources: run the following cells ONLY when all ETL processing is complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Delete IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted IAM Role Resources.\n"
     ]
    }
   ],
   "source": [
    "def deleteIamResources(iamRole):\n",
    "    '''Delete all created IAM resources.'''\n",
    "    iam.detach_role_policy(RoleName=iamRole, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "    iam.delete_role(RoleName=iamRole)\n",
    "    print('Deleted IAM Role Resources.')\n",
    "    return\n",
    "\n",
    "deleteIamResources(ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Delete redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster deletion in progress, checking status every 10 seconds.\n",
      "The cluster \"sparkifycluster\" no longer exists.\n"
     ]
    }
   ],
   "source": [
    "def deleteRedshiftResources(iamRole):\n",
    "    '''Delete created Redshift resources.'''\n",
    "    try:\n",
    "        print('Cluster deletion in progress, checking status every 10 seconds.')\n",
    "        redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "        while True:\n",
    "            time.sleep(10)\n",
    "            if not redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER):\n",
    "                break \n",
    "  \n",
    "    except Exception:\n",
    "    # except botocore.errorfactory.ClusterNotFoundFault:\n",
    "    # except botocore.exceptions.ClientError:\n",
    "        print('The cluster \"{}\" no longer exists.'.format(DWH_CLUSTER_IDENTIFIER))\n",
    "        \n",
    "deleteRedshiftResources(ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
